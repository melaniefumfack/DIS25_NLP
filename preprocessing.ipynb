{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Topic Gender Bias in ESUPOL (BTW17)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The Dask Engine for Modin is experimental.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#import pandas as pd\n",
    "os.environ['MODIN_ENGINE'] = 'dask'\n",
    "import modin.pandas as pd\n",
    "\n",
    "import string\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('german')\n",
    "\n",
    "from HanTa import HanoverTagger as ht\n",
    "tagger = ht.HanoverTagger('./morphmodel_ger.pgz')\n",
    "\n",
    "from genderize import Genderize\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "\n",
    "import re\n",
    "import glob\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns=500\n",
    "pd.options.display.width = 5000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "source_path = 'G:\\dis25'\n",
    "all_files = glob.glob(source_path + '/*.csv')[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load first slice of btw17 dataset into a Pandas DataFrame\n",
    "#### Load only the raw_data columns\n",
    "*the first slice was handled separately since it's the only one with headers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_4464/550357359.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m \u001B[1;33m**\u001B[0m \u001B[1;36m6\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mchunks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource_path\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'suggestions_20210719.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mchunksize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mchunksize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0musecols\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'raw_data'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mbtw17_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchunk\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mchunks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "chunksize = 10 ** 6\n",
    "chunks = pd.read_csv(os.path.join(source_path,'suggestions_20210719.csv'), chunksize=chunksize, usecols=['raw_data'])\n",
    "btw17_df = pd.concat(chunk for chunk in chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extract the raw data column and save th DataFrame to a separate file\n",
    "- *The queryterms were removed from the suggestions*\n",
    "- *The queryterms only include the firstname now*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "btw17_df = pd.DataFrame(btw17_df.raw_data.apply(lambda x: json.loads(x)).to_list(),columns=['queryterm','suggestions',3,4,5])[['queryterm','suggestions']]\n",
    "btw17_df['suggestions'] = btw17_df.apply(lambda x: [suggestion.replace(x.queryterm.lower(), '') for suggestion in x.suggestions], axis=1)\n",
    "btw17_df['queryterm'] = btw17_df.queryterm.apply(lambda x: x.split(' ')[0].split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "btw17_df = pd.to_csv('./raw_suggestions_20210719.csv', usecols=['queryterm','suggestions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the other Dataset slices, proceed as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chunksize = 10 ** 6\n",
    "for i in range(42,63):\n",
    "    chunks = pd.read_csv(all_files[i],\n",
    "                         header=None,\n",
    "                         names=['id','queryterm','date','client','lang','url','raw_data'],\n",
    "                         usecols=['raw_data'],\n",
    "                         chunksize=chunksize)\n",
    "    temp_df = pd.concat(chunks)\n",
    "\n",
    "    temp_df = pd.DataFrame(temp_df.raw_data.apply(lambda x: json.loads(x)).to_list(),columns=['queryterm','suggestions',3,4,5])[['queryterm','suggestions']]\n",
    "    temp_df['suggestions'] = temp_df.apply(lambda x: [suggestion.replace(x.queryterm.lower(), '').strip() for suggestion in x.suggestions], axis=1)\n",
    "    temp_df['queryterm'] = temp_df.queryterm.apply(lambda x: x.split(' ')[0].split('-')[0])\n",
    "\n",
    "\n",
    "    temp_df.to_csv('./raw_'+all_files[i].split('\\\\')[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get a list of all firstnames\n",
    "- *During the process duplicates were removed at multiple points to reduce computing efforts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_raw_df = glob.glob('./raw_suggestions*')\n",
    "name_list = []\n",
    "for i in all_raw_df:\n",
    "    chunks = pd.read_csv(i,\n",
    "                         usecols=['queryterm','suggestions'],\n",
    "                         chunksize=chunksize)\n",
    "    btw17_rawdata_df = btw17_rawdata_df.append(pd.concat(chunks))\n",
    "    name_list.extend(btw17_rawdata_df.queryterm.drop_duplicates().to_list())\n",
    "    name_list = list(set(name_list))\n",
    "print(len(name_list))\n",
    "name_list[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Genderize.io API\n",
    "> This API supports access to a database of hundreds of thousand names in various notations\n",
    "> It takes a list of name as input, sends a request to the Genderize.io server and delivers a\n",
    "> list containing a dictionary for each requested name listing a count, the gender (male/female/None) and a probability.\n",
    ">\n",
    "> However the number of requests per day is limited to 1000 (per IP address).\n",
    "> This isn't a problem for us though since we've got 326 unique names in the btw17 dataset\n",
    "> and XX unique names in the eu dataset.\n",
    "\n",
    "### Get gender for each name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gender_list_btw17 = Genderize().get(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count': 2406, 'gender': 'male', 'name': 'Gunnar', 'probability': 0.99},\n",
      " {'count': 755, 'gender': 'male', 'name': 'Ates', 'probability': 0.9},\n",
      " {'count': 1, 'gender': 'male', 'name': 'bundeskanzlerin', 'probability': 1.0},\n",
      " {'count': 245870, 'gender': 'male', 'name': 'Michael', 'probability': 0.99},\n",
      " {'count': 16593, 'gender': 'male', 'name': 'Maurice', 'probability': 0.98},\n",
      " {'count': 787, 'gender': 'male', 'name': 'Shen', 'probability': 0.65},\n",
      " {'count': 57806, 'gender': 'male', 'name': 'Florian', 'probability': 0.99},\n",
      " {'count': 38513, 'gender': 'female', 'name': 'Renata', 'probability': 0.99},\n",
      " {'count': 501011, 'gender': 'male', 'name': 'David', 'probability': 0.99},\n",
      " {'count': 1, 'gender': 'male', 'name': 'jungeunion', 'probability': 1.0},\n",
      " {'count': 11677, 'gender': 'male', 'name': 'Franz', 'probability': 0.97},\n",
      " {'count': 1028, 'gender': 'male', 'name': 'von', 'probability': 0.75},\n",
      " {'count': 41244, 'gender': 'male', 'name': 'Jimmy', 'probability': 0.98},\n",
      " {'count': 12170, 'gender': 'male', 'name': 'Bernd', 'probability': 0.99},\n",
      " {'count': 0, 'gender': None, 'name': 'dobrindt', 'probability': 0.0},\n",
      " {'count': 58, 'gender': 'male', 'name': 'Sigmar', 'probability': 0.98},\n",
      " {'count': 4428, 'gender': 'male', 'name': 'Gerd', 'probability': 0.93},\n",
      " {'count': 1578, 'gender': 'female', 'name': 'Sybille', 'probability': 0.98},\n",
      " {'count': 229433, 'gender': 'female', 'name': 'Marie', 'probability': 0.98},\n",
      " {'count': 45829, 'gender': 'male', 'name': 'Rafael', 'probability': 0.99},\n",
      " {'count': 9, 'gender': 'male', 'name': 'Bartholomäus', 'probability': 1.0},\n",
      " {'count': 19741, 'gender': 'female', 'name': 'Judith', 'probability': 0.98},\n",
      " {'count': 1012, 'gender': 'male', 'name': 'Stev', 'probability': 0.97},\n",
      " {'count': 245, 'gender': 'male', 'name': 'Volkmar', 'probability': 0.99},\n",
      " {'count': 4249, 'gender': 'male', 'name': 'Willi', 'probability': 0.98}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(gender_list_btw17[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Store results in a DataFrame and save to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "btw17_gender_df = pd.DataFrame(gender_list_btw17)\n",
    "btw17_gender_df = btw17_gender_df.rename(columns = {'name':'f_name','probability':'gender_probability'})\n",
    "btw17_gender_df = btw17_gender_df[['f_name','gender','gender_probability']]\n",
    "btw17_gender_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `to_csv` defaulting to pandas implementation.\n"
     ]
    }
   ],
   "source": [
    "btw17_gender_df.to_csv('btw17_name_gender_df.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Merge gender data with btw17 data and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>suggestions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>[' privat', 'http://www.jan-van-aken.de/', ' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>[' privat', ' bundestag', ' biografie', ' twit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>['', ' privat', ' biografie', ' bundestag', ' ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender                                        suggestions\n",
       "0   male  [' privat', 'http://www.jan-van-aken.de/', ' b...\n",
       "1   male  [' privat', ' bundestag', ' biografie', ' twit...\n",
       "2   male  ['', ' privat', ' biografie', ' bundestag', ' ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btw17_rawdata_df = btw17_rawdata_df.merge(btw17_gender_df, left_on = 'queryterm', right_on = 'f_name')[['gender','suggestions']]\n",
    "btw17_rawdata_df = btw17_rawdata_df[~btw17_rawdata_df.gender.isnull()]\n",
    "btw17_rawdata_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "btw17_rawdata_df = pd.to_csv('./btw17_rawdata_gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# chunksize= 10 ** 6\n",
    "# chunks = pd.read_csv('../dis25-2021/btw17_rawdata_gender.csv', usecols=['suggestions'], chunksize=chunksize)\n",
    "# btw17_rawdata_df = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get a list of all suggestion terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64875\n"
     ]
    },
    {
     "data": {
      "text/plain": "['',\n 'sen kiel',\n 'lunzenau',\n 'nicole höchst rede',\n 'anja karliczek best of',\n 'the name karin',\n 'enkopf',\n 'meckelfeld',\n 'klaus peter brähmig',\n 'lars kerfin general manager']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "suggestion_list = []\n",
    "suggestion_list = [item.strip() for sublist in btw17_rawdata_df.suggestions.drop_duplicates().to_list() for item in ast.literal_eval(sublist)]\n",
    "suggestion_list = list(set(suggestion_list))\n",
    "print(len(suggestion_list))\n",
    "\n",
    "suggestion_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dataset\n",
    "## Preprocessing Pipeline\n",
    ">*Flatten the nested list to get one big list of suggestion terms*\n",
    ">\n",
    ">**Preprocessing Pipeline:**\n",
    ">- Remove punctuation\n",
    ">- Remove names from the suggestion terms to get objective terms\n",
    ">   - This includes person names and location names\n",
    ">- Only use unigrams\n",
    ">- Replace umlauts\n",
    ">- Remove digits\n",
    ">- Set strings to lowercase\n",
    ">- Remove urls\n",
    ">- Remove nltk stopwords\n",
    ">- Remove terms that consist of only 2 chars or less\n",
    ">- Strip whitespaces\n",
    ">- Remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_names(matchobj):\n",
    "    word = matchobj.group(0)\n",
    "    if word in names:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return word\n",
    "word_pattern = re.compile('\\w+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Sources of files used in preprocessing pipeline*:\n",
    "* list.txt\n",
    "    * https://github.com/FinNLP/cities-list\n",
    "* csv-data.csv\n",
    "    * https://www.auswaertiges-amt.de/de/service/terminologie/-/215252\n",
    "* countries.txt\n",
    "    *  https://gist.github.com/kalinchernev/486393efcca01623b18d\n",
    "* male.txt\n",
    "    * https://www.kaggle.com/nltkdata/names\n",
    "* female.txt\n",
    "    * https://www.kaggle.com/nltkdata/names\n",
    "* usna_names.txt\n",
    "    * https://www.usna.edu/Users/cs/roche/courses/s15si335/proj1/files.php%3Ff=names.txt.html\n",
    "* wordlist_german.txt\n",
    "    * https://gist.github.com/MarvinJWendt/2f4f4154b8ae218600eb091a5706b5f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('./list.txt', 'r') as f:\n",
    "    locations = [(term[:-1].lower()) for term in f.readlines()]\n",
    "countries = pd.read_csv('./csv-data.csv', delimiter=';', encoding='windows-1252')\n",
    "locations.extend([(term.lower()) for term in countries['Kurzform'].to_list()])\n",
    "with open('./countries.txt', 'r') as f:\n",
    "    locations.extend([(term[:-1].lower()) for term in f.readlines()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk_names = []\n",
    "with open('./male.txt') as male_names_nltk:\n",
    "    nltk_names = [name.lower() for name in male_names_nltk.read().split('\\n')]\n",
    "with open('./female.txt') as female_names_nltk:\n",
    "    nltk_names.extend([name.lower() for name in female_names_nltk.read().split('\\n')])\n",
    "print(nltk_names[:20])\n",
    "print(nltk_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "usna_names =[]\n",
    "with open('./names.txt') as usna_file:\n",
    "    usna_names = [name.lower() for name in usna_file.read().split('\\n')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "chunksize = 10 ** 6\n",
    "chunks = pd.read_csv(os.path.join(source_path,'suggestions_20210719.csv'), chunksize=chunksize, usecols=['queryterm'])\n",
    "btw17_df = pd.concat(chunk for chunk in chunks)\n",
    "names.extend(btw17_df.queryterm.drop_duplicates().to_list())\n",
    "\n",
    "chunksize = 10 ** 6\n",
    "for i in all_files:\n",
    "    chunks = pd.read_csv(i,\n",
    "                         header=None,\n",
    "                         names=['id','queryterm','date','client','lang','url','raw_data'],\n",
    "                         usecols=['queryterm'],\n",
    "                         chunksize=chunksize)\n",
    "    temp_df = pd.concat(chunks)\n",
    "    names.extend(temp_df.queryterm.drop_duplicates().to_list())\n",
    "\n",
    "names = list(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## get a list of names\n",
    "names = [subname.strip('()').lower() for name in names for subname in name.split() if len(subname) > 2]\n",
    "names.extend(nltk_names)\n",
    "names.extend(usna_names)\n",
    "names.extend(locations)\n",
    "names = list(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'pandacaqui', 'fuling', 'bunbury', 'charitie', 'markale', 'cudworth', 'auchinleck', 'neos marmaras', 'sara-ann', 'sueanne', 'wingene', 'golda', 'maximilien', 'xaltianguis', 'zimandu nou', 'guaranesia', 'san mateo ixtatan', 'puyallup', 'dustina']\n"
     ]
    }
   ],
   "source": [
    "with open('./all_names_to_delete.txt', 'r',encoding='utf-8') as f:\n",
    "    names = [term[:-1] for term in f.readlines()]\n",
    "print(names[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load german dictionary file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aachen', 'aachener', 'aachenerin', 'aachenerinnen', 'aachenern', 'aacheners', 'aachens', 'aal', 'aalähnlich', 'aalähnliche', 'aalähnlichem', 'aalähnlichen', 'aalähnlicher', 'aalähnliches', 'aalangelfischerei', 'aalangeln', 'aalangelns', 'aalartig']\n"
     ]
    }
   ],
   "source": [
    "with open('../dis25-2021/wordlist-german.txt', 'r', encoding='utf-8') as f:\n",
    "    german_terms = [term[:-1].lower() for term in f.readlines()]\n",
    "print(german_terms[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define preprocessing pipeline\n",
    "def preprocess(suggestions:list)->list:\n",
    "    suggestions = tuple(map(lambda x: x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))),suggestions))\n",
    "    suggestions = [word_pattern.sub(delete_names, term) for term in suggestions]\n",
    "    suggestions = [suggestion for suggestion in suggestions if len(suggestion.split()) == 1]\n",
    "    suggestions = tuple(set(suggestions))\n",
    "    suggestions = tuple(map(lambda x: re.sub('[0-9]*', '', x), suggestions))\n",
    "    suggestions = [x.lower() for x in suggestions]\n",
    "    suggestions = [x for x in suggestions if not 'http' in x]\n",
    "    suggestions = [x for x in suggestions if not x in stopwords]\n",
    "    suggestions = [tagger.analyze(tagger.analyze(suggestion)[0])[0].lower() for suggestion in suggestions if tagger.analyze(tagger.analyze(suggestion)[0])[1] in ['NN','VVFIN','VVINF','NE']]\n",
    "    suggestions = [re.sub('(^..?\\s|\\s..?\\s|\\s..?$|^..?$)', '', string) for string in suggestions]\n",
    "    suggestions = [x.strip() for x in suggestions]\n",
    "    suggestions = list(filter(None, suggestions))\n",
    "    suggestions = list(set(suggestions))\n",
    "    suggestions = [term for term in suggestions if term in german_terms]\n",
    "    return(suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Perform Preprocessing and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "suggestions = preprocess(suggestion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gi', 'fußfessel', 'aau', 'fluids', 'meyering', 'laudatio', 'schwörstadt', 'enkopf', 'northoff', 'meckelfeld']\n"
     ]
    },
    {
     "data": {
      "text/plain": "18022"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(suggestions[:10])\n",
    "len(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "textfile = open(\"./suggestion_terms_prep.txt\", \"w\",encoding=\"utf-8\")\n",
    "for element in suggestions:\n",
    "    textfile. write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}